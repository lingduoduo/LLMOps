export PHOENIX_ENABLE_AUTH=True
export PHOENIX_SECRET=3413f9a7735bb780c6b8e4db7d946a492b64d26112a955cdea6a797f4c833593

https://arize.com/generative-ai/

https://arize.com/docs/ax

https://arize.com/model-monitoring/

https://arize.com/?post_type=course&p=14924

https://colab.research.google.com/drive/1FIJLgqQrd255-cLIKgTeU2ToTQhRQuv9?usp=sharing

https://colab.research.google.com/github/Arize-ai/tutorials/blob/main/python/llm/experiments/summarization-experiment.ipynb

https://colab.research.google.com/github/Arize-ai/tutorials/blob/main/python/llm/experiments/summarization-experiment.ipynb#scrollTo=T7QYiS3gl40N
SPACE_ID
API_KEY

- [Arize Documentation](https://docs.arize.com/phoenix/notebooks)
- [LLM Evaluation: The Definitive Guide](https://arize.com/blog-course/llm-evaluation-the-definitive-guide/)
- [LLM Evaluation: Assessing Large Language Models Using Their Peers](https://arize.com/blog-course/assessing-large-language-models/)
- [Navigating LLM Leaderboards for Model Selection: An Overview of Benchmark Evaluations](https://arize.com/blog-course/llm-leaderboards-benchmarks/)
- [Evals from OpenAI: Simplifying and Streamlining LLM Evaluation](https://arize.com/blog-course/evals-openai-simplifying-llm-evaluation/)

Github Repos:

- [Phoenix evals](https://github.com/Arize-ai/phoenix/tree/main/tutorials/evals)
- [Resources for Evaluation of LLMs / Generative AI](https://github.com/rajshah4/LLM-Evaluation#resources-for-evaluation-of-llms--generative-ai)
