{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Using OpenAI models with Strands Agent\n",
    "\n",
    "## Overview\n",
    "\n",
    "Strands Agents is an SDK that takes a model-driven approach to building and running AI agents in just a few lines of code. Strands supports multiple providers and models hosted anywhere.\n",
    "\n",
    "[LiteLLM](https://docs.litellm.ai/docs/) is a unified interface for various LLM providers that allows you to interact with models from Amazon, Anthropic, OpenAI, and many others through a single API. The Strands Agent SDK implements a LiteLLM provider, allowing you to run agents against any model LiteLLM supports.\n",
    "\n",
    "In this example, we will show you how to use `gpt-4.1-mini` model hosted in Microsoft Azure as the underlying model in your Strands Agent. We will use a simple agent use case with a weather and a get time tool.\n",
    "\n",
    "\n",
    "## Agent Details\n",
    "<div style=\"float: left; margin-right: 20px;\">\n",
    "    \n",
    "|Feature             |Description                                        |\n",
    "|--------------------|---------------------------------------------------|\n",
    "|Feature used        |LiteLLM model                                      |\n",
    "|Agent Structure     |Single agent architecture                          |\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/architecture.png\" width=\"65%\" />\n",
    "</div>\n",
    "\n",
    "## Key Features\n",
    "* **LiteLLM model**: using a model provided via LiteLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and prerequisites\n",
    "\n",
    "### Prerequisites\n",
    "* Python 3.10+\n",
    "* Azure Account\n",
    "* gpt-4.1-mini access\n",
    "\n",
    "Let's now install the requirement packages for our Strands Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T20:19:43.409290Z",
     "iopub.status.busy": "2025-07-30T20:19:43.408997Z",
     "iopub.status.idle": "2025-07-30T20:19:47.944423Z",
     "shell.execute_reply": "2025-07-30T20:19:47.943805Z",
     "shell.execute_reply.started": "2025-07-30T20:19:43.409269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: strands-agents in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: strands-agents-tools in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: aiohttp>=3.12.13 in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (3.12.13)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (1.37.1)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.29.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (1.37.38)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (0.17.0)\n",
      "Requirement already satisfied: mcp<2.0.0,>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (1.12.2)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.30.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-threading<1.00b0,>=0.51b0 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (0.57b0)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.30.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (1.36.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (2.11.7)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.13.2 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (4.14.1)\n",
      "Requirement already satisfied: watchdog<7.0.0,>=6.0.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (6.0.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from boto3<2.0.0,>=1.26.0->strands-agents->-r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /opt/conda/lib/python3.12/site-packages (from boto3<2.0.0,>=1.26.0->strands-agents->-r requirements.txt (line 1)) (0.11.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.12/site-packages (from botocore<2.0.0,>=1.29.0->strands-agents->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.12/site-packages (from botocore<2.0.0,>=1.29.0->strands-agents->-r requirements.txt (line 1)) (1.26.19)\n",
      "Requirement already satisfied: anyio>=4.5 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: httpx>=0.27 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (4.23.0)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (2.10.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (0.0.20)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (3.0.2)\n",
      "Requirement already satisfied: starlette>=0.27 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (0.46.2)\n",
      "Requirement already satisfied: uvicorn>=0.23.1 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (0.35.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-api<2.0.0,>=1.30.0->strands-agents->-r requirements.txt (line 1)) (6.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.30.0->strands-agents->-r requirements.txt (line 1)) (3.23.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.57b0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents->-r requirements.txt (line 1)) (0.57b0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents->-r requirements.txt (line 1)) (1.17.2)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-instrumentation==0.57b0->opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents->-r requirements.txt (line 1)) (0.57b0)\n",
      "Requirement already satisfied: packaging>=18.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-instrumentation==0.57b0->opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents->-r requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->strands-agents->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->strands-agents->-r requirements.txt (line 1)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->strands-agents->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.29.0->strands-agents->-r requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: aws-requests-auth<0.5.0,>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (0.4.3)\n",
      "Requirement already satisfied: dill<0.5.0,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (0.4.0)\n",
      "Requirement already satisfied: markdownify<2.0.0,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (1.1.0)\n",
      "Requirement already satisfied: pillow<12.0.0,>=11.2.1 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (11.3.0)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.51 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (3.0.51)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.10.1 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (2.10.1)\n",
      "Requirement already satisfied: readabilipy<1.0.0,>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (0.3.0)\n",
      "Requirement already satisfied: rich<15.0.0,>=14.0.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (14.1.0)\n",
      "Requirement already satisfied: slack-bolt<2.0.0,>=1.23.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (1.23.0)\n",
      "Requirement already satisfied: sympy<2.0.0,>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (1.14.0)\n",
      "Requirement already satisfied: tenacity<10.0.0,>=9.1.2 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (9.1.2)\n",
      "Requirement already satisfied: requests>=0.14.0 in /opt/conda/lib/python3.12/site-packages (from aws-requests-auth<0.5.0,>=0.4.3->strands-agents-tools->-r requirements.txt (line 2)) (2.32.4)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.9 in /opt/conda/lib/python3.12/site-packages (from markdownify<2.0.0,>=1.0.0->strands-agents-tools->-r requirements.txt (line 2)) (4.13.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.12/site-packages (from beautifulsoup4<5,>=4.9->markdownify<2.0.0,>=1.0.0->strands-agents-tools->-r requirements.txt (line 2)) (2.7)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.12/site-packages (from prompt-toolkit<4.0.0,>=3.0.51->strands-agents-tools->-r requirements.txt (line 2)) (0.2.13)\n",
      "Requirement already satisfied: html5lib in /opt/conda/lib/python3.12/site-packages (from readabilipy<1.0.0,>=0.2.0->strands-agents-tools->-r requirements.txt (line 2)) (1.1)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.12/site-packages (from readabilipy<1.0.0,>=0.2.0->strands-agents-tools->-r requirements.txt (line 2)) (5.4.0)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.12/site-packages (from readabilipy<1.0.0,>=0.2.0->strands-agents-tools->-r requirements.txt (line 2)) (2024.11.6)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich<15.0.0,>=14.0.0->strands-agents-tools->-r requirements.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich<15.0.0,>=14.0.0->strands-agents-tools->-r requirements.txt (line 2)) (2.19.2)\n",
      "Requirement already satisfied: slack_sdk<4,>=3.35.0 in /opt/conda/lib/python3.12/site-packages (from slack-bolt<2.0.0,>=1.23.0->strands-agents-tools->-r requirements.txt (line 2)) (3.36.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy<2.0.0,>=1.12.0->strands-agents-tools->-r requirements.txt (line 2)) (1.3.0)\n",
      "Collecting litellm<1.73.0,>=1.72.6 (from strands-agents[litellm]->-r requirements.txt (line 3))\n",
      "  Downloading litellm-1.72.9-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from litellm<1.73.0,>=1.72.6->strands-agents[litellm]->-r requirements.txt (line 3)) (8.2.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /opt/conda/lib/python3.12/site-packages (from litellm<1.73.0,>=1.72.6->strands-agents[litellm]->-r requirements.txt (line 3)) (3.1.6)\n",
      "Collecting openai>=1.68.2 (from litellm<1.73.0,>=1.72.6->strands-agents[litellm]->-r requirements.txt (line 3))\n",
      "  Downloading openai-1.98.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from litellm<1.73.0,>=1.72.6->strands-agents[litellm]->-r requirements.txt (line 3)) (1.1.1)\n",
      "Collecting tiktoken>=0.7.0 (from litellm<1.73.0,>=1.72.6->strands-agents[litellm]->-r requirements.txt (line 3))\n",
      "  Downloading tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tokenizers in /opt/conda/lib/python3.12/site-packages (from litellm<1.73.0,>=1.72.6->strands-agents[litellm]->-r requirements.txt (line 3)) (0.21.4.dev0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2<4.0.0,>=3.1.2->litellm<1.73.0,>=1.72.6->strands-agents[litellm]->-r requirements.txt (line 3)) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (0.26.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp>=3.12.13->-r requirements.txt (line 4)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp>=3.12.13->-r requirements.txt (line 4)) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp>=3.12.13->-r requirements.txt (line 4)) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp>=3.12.13->-r requirements.txt (line 4)) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp>=3.12.13->-r requirements.txt (line 4)) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp>=3.12.13->-r requirements.txt (line 4)) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp>=3.12.13->-r requirements.txt (line 4)) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.12/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx>=0.27->mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx>=0.27->mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=14.0.0->strands-agents-tools->-r requirements.txt (line 2)) (0.1.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.12/site-packages (from openai>=1.68.2->litellm<1.73.0,>=1.72.6->strands-agents[litellm]->-r requirements.txt (line 3)) (1.9.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.68.2->litellm<1.73.0,>=1.72.6->strands-agents[litellm]->-r requirements.txt (line 3))\n",
      "  Downloading jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.12/site-packages (from openai>=1.68.2->litellm<1.73.0,>=1.72.6->strands-agents[litellm]->-r requirements.txt (line 3)) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=0.14.0->aws-requests-auth<0.5.0,>=0.4.3->strands-agents-tools->-r requirements.txt (line 2)) (3.4.2)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.12/site-packages (from html5lib->readabilipy<1.0.0,>=0.2.0->strands-agents-tools->-r requirements.txt (line 2)) (0.5.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.12/site-packages (from tokenizers->litellm<1.73.0,>=1.72.6->strands-agents[litellm]->-r requirements.txt (line 3)) (0.33.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm<1.73.0,>=1.72.6->strands-agents[litellm]->-r requirements.txt (line 3)) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm<1.73.0,>=1.72.6->strands-agents[litellm]->-r requirements.txt (line 3)) (2024.12.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm<1.73.0,>=1.72.6->strands-agents[litellm]->-r requirements.txt (line 3)) (1.1.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm<1.73.0,>=1.72.6->strands-agents[litellm]->-r requirements.txt (line 3)) (6.0.2)\n",
      "Downloading litellm-1.72.9-py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m154.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.98.0-py3-none-any.whl (767 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m767.7/767.7 kB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Downloading tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m144.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: jiter, tiktoken, openai, litellm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [litellm]m3/4\u001b[0m [litellm]\n",
      "\u001b[1A\u001b[2KSuccessfully installed jiter-0.10.0 litellm-1.72.9 openai-1.98.0 tiktoken-0.9.0\n"
     ]
    }
   ],
   "source": [
    "# installing pre-requisites\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dependency packages\n",
    "\n",
    "Now let's import the dependency packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T20:19:47.945975Z",
     "iopub.status.busy": "2025-07-30T20:19:47.945474Z",
     "iopub.status.idle": "2025-07-30T20:19:49.910401Z",
     "shell.execute_reply": "2025-07-30T20:19:49.909892Z",
     "shell.execute_reply.started": "2025-07-30T20:19:47.945949Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from datetime import timezone as tz\n",
    "from typing import Any\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "from strands import Agent, tool\n",
    "from strands.models.litellm import LiteLLMModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Azure keys\n",
    "\n",
    "Let's now setup the Azure API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T20:19:49.911351Z",
     "iopub.status.busy": "2025-07-30T20:19:49.911077Z",
     "iopub.status.idle": "2025-07-30T20:19:49.914265Z",
     "shell.execute_reply": "2025-07-30T20:19:49.913822Z",
     "shell.execute_reply.started": "2025-07-30T20:19:49.911332Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"AZURE_API_KEY\"] = \"<YOUR_API_KEY>\"\n",
    "os.environ[\"AZURE_API_BASE\"] = \"<YOUR_API_BASE>\"\n",
    "os.environ[\"AZURE_API_VERSION\"] = \"<YOUR_API_VERSION>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up custom tools\n",
    "\n",
    "Let's now setup two dummy tools to test our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T20:19:49.915474Z",
     "iopub.status.busy": "2025-07-30T20:19:49.915200Z",
     "iopub.status.idle": "2025-07-30T20:19:49.923220Z",
     "shell.execute_reply": "2025-07-30T20:19:49.922764Z",
     "shell.execute_reply.started": "2025-07-30T20:19:49.915458Z"
    }
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def current_time(timezone: str = \"UTC\") -> str:\n",
    "    if timezone.upper() == \"UTC\":\n",
    "        timezone_obj: Any = tz.utc\n",
    "    else:\n",
    "        timezone_obj = ZoneInfo(timezone)\n",
    "\n",
    "    return datetime.now(timezone_obj).isoformat()\n",
    "\n",
    "\n",
    "@tool\n",
    "def current_weather(city: str) -> str:\n",
    "    # Dummy implementation. Please replace with actual weather API call.\n",
    "    return \"sunny\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining agent underlying LLM model\n",
    "\n",
    "Next let's define our agent underlying model using LiteLLM. We will set it to `gpt-4.1-mini`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T20:19:49.923982Z",
     "iopub.status.busy": "2025-07-30T20:19:49.923714Z",
     "iopub.status.idle": "2025-07-30T20:19:49.926426Z",
     "shell.execute_reply": "2025-07-30T20:19:49.925971Z",
     "shell.execute_reply.started": "2025-07-30T20:19:49.923954Z"
    }
   },
   "outputs": [],
   "source": [
    "model = \"azure/gpt-4.1-mini\"\n",
    "litellm_model = LiteLLMModel(\n",
    "    model_id=model, params={\"max_tokens\": 32000, \"temperature\": 0.7}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Agent\n",
    "\n",
    "Now that we have all the required information available, let's define our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T20:19:49.927136Z",
     "iopub.status.busy": "2025-07-30T20:19:49.926933Z",
     "iopub.status.idle": "2025-07-30T20:19:49.929886Z",
     "shell.execute_reply": "2025-07-30T20:19:49.929484Z",
     "shell.execute_reply.started": "2025-07-30T20:19:49.927121Z"
    }
   },
   "outputs": [],
   "source": [
    "system_prompt = \"You are a simple agent that can tell the time and the weather\"\n",
    "agent = Agent(\n",
    "    model=litellm_model,\n",
    "    system_prompt=system_prompt,\n",
    "    tools=[current_time, current_weather],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing agent\n",
    "\n",
    "Let's now invoke the agent to test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T20:19:49.930512Z",
     "iopub.status.busy": "2025-07-30T20:19:49.930347Z",
     "iopub.status.idle": "2025-07-30T20:19:51.954113Z",
     "shell.execute_reply": "2025-07-30T20:19:51.953123Z",
     "shell.execute_reply.started": "2025-07-30T20:19:49.930497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "ename": "APIConnectionError",
     "evalue": "litellm.APIConnectionError: AzureException APIConnectionError - list index out of range\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.12/site-packages/litellm/main.py\", line 1194, in completion\n    optional_params = get_optional_params(\n                      ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.12/site-packages/litellm/utils.py\", line 3766, in get_optional_params\n    optional_params = litellm.AzureOpenAIConfig().map_openai_params(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.12/site-packages/litellm/llms/azure/chat/gpt_transformation.py\", line 165, in map_openai_params\n    api_version_month = api_version_times[1]\n                        ~~~~~~~~~~~~~~~~~^^^\nIndexError: list index out of range\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/litellm/main.py:1194\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[0m\n\u001b[1;32m   1158\u001b[0m optional_param_args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1159\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallowed_openai_params\u001b[39m\u001b[38;5;124m\"\u001b[39m: kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallowed_openai_params\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1193\u001b[0m }\n\u001b[0;32m-> 1194\u001b[0m optional_params \u001b[38;5;241m=\u001b[39m \u001b[43mget_optional_params\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptional_param_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnon_default_params\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1197\u001b[0m processed_non_default_params \u001b[38;5;241m=\u001b[39m pre_process_non_default_params(\n\u001b[1;32m   1198\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1199\u001b[0m     passed_params\u001b[38;5;241m=\u001b[39moptional_param_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1204\u001b[0m     add_provider_specific_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1205\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/litellm/utils.py:3766\u001b[0m, in \u001b[0;36mget_optional_params\u001b[0;34m(model, functions, function_call, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, custom_llm_provider, response_format, seed, tools, tool_choice, max_retries, logprobs, top_logprobs, extra_headers, api_version, parallel_tool_calls, drop_params, allowed_openai_params, reasoning_effort, additional_drop_params, messages, thinking, web_search_options, **kwargs)\u001b[0m\n\u001b[1;32m   3760\u001b[0m         api_version \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   3761\u001b[0m             api_version\n\u001b[1;32m   3762\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mapi_version\n\u001b[1;32m   3763\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m get_secret(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAZURE_API_VERSION\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3764\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mAZURE_DEFAULT_API_VERSION\n\u001b[1;32m   3765\u001b[0m         )\n\u001b[0;32m-> 3766\u001b[0m         optional_params \u001b[38;5;241m=\u001b[39m \u001b[43mlitellm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAzureOpenAIConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_openai_params\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3767\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnon_default_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_default_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3768\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3769\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3770\u001b[0m \u001b[43m            \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m   3771\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3772\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdrop_params\u001b[49m\n\u001b[1;32m   3773\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdrop_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdrop_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3774\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   3775\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3776\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3777\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m provider_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/litellm/llms/azure/chat/gpt_transformation.py:165\u001b[0m, in \u001b[0;36mAzureOpenAIConfig.map_openai_params\u001b[0;34m(self, non_default_params, optional_params, model, drop_params, api_version)\u001b[0m\n\u001b[1;32m    164\u001b[0m api_version_year \u001b[38;5;241m=\u001b[39m api_version_times[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 165\u001b[0m api_version_month \u001b[38;5;241m=\u001b[39m \u001b[43mapi_version_times\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    166\u001b[0m api_version_day \u001b[38;5;241m=\u001b[39m api_version_times[\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAPIConnectionError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat time is it in Seattle? And how is the weather?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/strands/agent/agent.py:377\u001b[0m, in \u001b[0;36mAgent.__call__\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m    376\u001b[0m     future \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39msubmit(execute)\n\u001b[0;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/concurrent/futures/_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/concurrent/futures/thread.py:59\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/opentelemetry/instrumentation/threading/__init__.py:171\u001b[0m, in \u001b[0;36mThreadingInstrumentor.__wrap_thread_pool_submit.<locals>.wrapped_func\u001b[0;34m(*func_args, **func_kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     token \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mattach(otel_context)\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moriginal_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/strands/agent/agent.py:373\u001b[0m, in \u001b[0;36mAgent.__call__.<locals>.execute\u001b[0;34m()\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexecute\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AgentResult:\n\u001b[0;32m--> 373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/asyncio/runners.py:195\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug, loop_factory)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug, loop_factory\u001b[38;5;241m=\u001b[39mloop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/asyncio/runners.py:118\u001b[0m, in \u001b[0;36mRunner.run\u001b[0;34m(self, coro, context)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interrupt_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCancelledError:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interrupt_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/asyncio/base_events.py:691\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m future\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 691\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/strands/agent/agent.py:398\u001b[0m, in \u001b[0;36mAgent.invoke_async\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Process a natural language prompt through the agent's event loop.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \n\u001b[1;32m    382\u001b[0m \u001b[38;5;124;03mThis method implements the conversational interface (e.g., `agent(\"hello!\")`). It adds the user's prompt to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;124;03m        - state: The final state of the event loop\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    397\u001b[0m events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_async(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 398\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m events:\n\u001b[1;32m    399\u001b[0m     _ \u001b[38;5;241m=\u001b[39m event\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(AgentResult, event[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/strands/agent/agent.py:508\u001b[0m, in \u001b[0;36mAgent.stream_async\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    507\u001b[0m     events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_loop(message, invocation_state\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 508\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m events:\n\u001b[1;32m    509\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m event:\n\u001b[1;32m    510\u001b[0m             callback_handler(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mevent[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/strands/agent/agent.py:544\u001b[0m, in \u001b[0;36mAgent._run_loop\u001b[0;34m(self, message, invocation_state)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;66;03m# Execute the event loop cycle with retry logic for context limits\u001b[39;00m\n\u001b[1;32m    543\u001b[0m events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_event_loop_cycle(invocation_state)\n\u001b[0;32m--> 544\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m events:\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;66;03m# Signal from the model provider that the message sent by the user should be redacted,\u001b[39;00m\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;66;03m# likely due to a guardrail.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    548\u001b[0m         event\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    549\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m event[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m event[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mredactContent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m event[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mredactContent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mredactUserContentMessage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    552\u001b[0m     ):\n\u001b[1;32m    553\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    554\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: event[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mredactContent\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mredactUserContentMessage\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m    555\u001b[0m         ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/strands/agent/agent.py:583\u001b[0m, in \u001b[0;36mAgent._execute_event_loop_cycle\u001b[0;34m(self, invocation_state)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;66;03m# Execute the main event loop cycle\u001b[39;00m\n\u001b[1;32m    579\u001b[0m     events \u001b[38;5;241m=\u001b[39m event_loop_cycle(\n\u001b[1;32m    580\u001b[0m         agent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    581\u001b[0m         invocation_state\u001b[38;5;241m=\u001b[39minvocation_state,\n\u001b[1;32m    582\u001b[0m     )\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m events:\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m event\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ContextWindowOverflowException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;66;03m# Try reducing the context size and retrying\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/strands/event_loop/event_loop.py:187\u001b[0m, in \u001b[0;36mevent_loop_cycle\u001b[0;34m(agent, invocation_state)\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[38;5;28;01myield\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent_loop_throttled_delay\u001b[39m\u001b[38;5;124m\"\u001b[39m: current_delay, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minvocation_state}}\n\u001b[1;32m    186\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;66;03m# Add message in trace and mark the end of the stream messages trace\u001b[39;00m\n\u001b[1;32m    191\u001b[0m     stream_trace\u001b[38;5;241m.\u001b[39madd_message(message)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/strands/event_loop/event_loop.py:132\u001b[0m, in \u001b[0;36mevent_loop_cycle\u001b[0;34m(agent, invocation_state)\u001b[0m\n\u001b[1;32m    122\u001b[0m agent\u001b[38;5;241m.\u001b[39mhooks\u001b[38;5;241m.\u001b[39minvoke_callbacks(\n\u001b[1;32m    123\u001b[0m     BeforeModelInvocationEvent(\n\u001b[1;32m    124\u001b[0m         agent\u001b[38;5;241m=\u001b[39magent,\n\u001b[1;32m    125\u001b[0m     )\n\u001b[1;32m    126\u001b[0m )\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# TODO: To maintain backwards compatibility, we need to combine the stream event with invocation_state\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m#       before yielding to the callback handler. This will be revisited when migrating to strongly\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m#       typed events.\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m stream_messages(agent\u001b[38;5;241m.\u001b[39mmodel, agent\u001b[38;5;241m.\u001b[39msystem_prompt, agent\u001b[38;5;241m.\u001b[39mmessages, tool_specs):\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m event:\n\u001b[1;32m    134\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m {\n\u001b[1;32m    135\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    136\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mevent[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    137\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(invocation_state \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m event[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[1;32m    138\u001b[0m                 }\n\u001b[1;32m    139\u001b[0m             }\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/strands/event_loop/streaming.py:318\u001b[0m, in \u001b[0;36mstream_messages\u001b[0;34m(model, system_prompt, messages, tool_specs)\u001b[0m\n\u001b[1;32m    314\u001b[0m messages \u001b[38;5;241m=\u001b[39m remove_blank_messages_content_text(messages)\n\u001b[1;32m    316\u001b[0m chunks \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mstream(messages, tool_specs \u001b[38;5;28;01mif\u001b[39;00m tool_specs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, system_prompt)\n\u001b[0;32m--> 318\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m process_stream(chunks):\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/strands/event_loop/streaming.py:273\u001b[0m, in \u001b[0;36mprocess_stream\u001b[0;34m(chunks)\u001b[0m\n\u001b[1;32m    270\u001b[0m usage: Usage \u001b[38;5;241m=\u001b[39m Usage(inputTokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, outputTokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, totalTokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    271\u001b[0m metrics: Metrics \u001b[38;5;241m=\u001b[39m Metrics(latencyMs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 273\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk}}\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessageStart\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m chunk:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/strands/models/litellm.py:130\u001b[0m, in \u001b[0;36mLiteLLMModel.stream\u001b[0;34m(self, messages, tool_specs, system_prompt, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest=<\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m, request)\n\u001b[1;32m    129\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvoking model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 130\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39macompletion(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest)\n\u001b[1;32m    132\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot response from model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_chunk({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage_start\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/litellm/utils.py:1495\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper_async\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1493\u001b[0m timeout \u001b[38;5;241m=\u001b[39m _get_wrapper_timeout(kwargs\u001b[38;5;241m=\u001b[39mkwargs, exception\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m   1494\u001b[0m \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout)\n\u001b[0;32m-> 1495\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/litellm/utils.py:1356\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper_async\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _caching_handler_response\u001b[38;5;241m.\u001b[39mfinal_embedding_cached_response\n\u001b[1;32m   1355\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[0;32m-> 1356\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m original_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1357\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/litellm/main.py:544\u001b[0m, in \u001b[0;36macompletion\u001b[0;34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    543\u001b[0m     custom_llm_provider \u001b[38;5;241m=\u001b[39m custom_llm_provider \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_type(\n\u001b[1;32m    545\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    546\u001b[0m         custom_llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m    547\u001b[0m         original_exception\u001b[38;5;241m=\u001b[39me,\n\u001b[1;32m    548\u001b[0m         completion_kwargs\u001b[38;5;241m=\u001b[39mcompletion_kwargs,\n\u001b[1;32m    549\u001b[0m         extra_kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m    550\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/litellm/main.py:517\u001b[0m, in \u001b[0;36macompletion\u001b[0;34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m ctx \u001b[38;5;241m=\u001b[39m contextvars\u001b[38;5;241m.\u001b[39mcopy_context()\n\u001b[1;32m    515\u001b[0m func_with_context \u001b[38;5;241m=\u001b[39m partial(ctx\u001b[38;5;241m.\u001b[39mrun, func)\n\u001b[0;32m--> 517\u001b[0m init_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mrun_in_executor(\u001b[38;5;28;01mNone\u001b[39;00m, func_with_context)\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(init_response, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    519\u001b[0m     init_response, ModelResponse\n\u001b[1;32m    520\u001b[0m ):  \u001b[38;5;66;03m## CACHING SCENARIO\u001b[39;00m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(init_response, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/concurrent/futures/thread.py:59\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/opentelemetry/instrumentation/threading/__init__.py:171\u001b[0m, in \u001b[0;36mThreadingInstrumentor.__wrap_thread_pool_submit.<locals>.wrapped_func\u001b[0;34m(*func_args, **func_kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     token \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mattach(otel_context)\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moriginal_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/litellm/utils.py:1021\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax retries per request hit!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[0;32m-> 1021\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1024\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplete_response\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs\n\u001b[1;32m   1025\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplete_response\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m     ):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/litellm/main.py:3305\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[0m\n\u001b[1;32m   3302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m   3303\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   3304\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[0;32m-> 3305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3308\u001b[0m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3311\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2276\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   2274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, error_type):\n\u001b[1;32m   2275\u001b[0m         \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, litellm_response_headers)\n\u001b[0;32m-> 2276\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e  \u001b[38;5;66;03m# it's already mapped\u001b[39;00m\n\u001b[1;32m   2277\u001b[0m raised_exc \u001b[38;5;241m=\u001b[39m APIConnectionError(\n\u001b[1;32m   2278\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(original_exception, traceback\u001b[38;5;241m.\u001b[39mformat_exc()),\n\u001b[1;32m   2279\u001b[0m     llm_provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2280\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2281\u001b[0m )\n\u001b[1;32m   2282\u001b[0m \u001b[38;5;28msetattr\u001b[39m(raised_exc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, litellm_response_headers)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2121\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   2109\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m APIError(\n\u001b[1;32m   2110\u001b[0m                 status_code\u001b[38;5;241m=\u001b[39moriginal_exception\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[1;32m   2111\u001b[0m                 message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAzureException APIError - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2117\u001b[0m                 ),\n\u001b[1;32m   2118\u001b[0m             )\n\u001b[1;32m   2119\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2120\u001b[0m         \u001b[38;5;66;03m# if no status code then it is an APIConnectionError: https://github.com/openai/openai-python#handling-errors\u001b[39;00m\n\u001b[0;32m-> 2121\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(\n\u001b[1;32m   2122\u001b[0m             message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m APIConnectionError - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2123\u001b[0m             llm_provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazure\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2124\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   2125\u001b[0m             litellm_debug_info\u001b[38;5;241m=\u001b[39mextra_information,\n\u001b[1;32m   2126\u001b[0m             request\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mRequest(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://openai.com/\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   2127\u001b[0m         )\n\u001b[1;32m   2128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m custom_llm_provider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenrouter\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(original_exception, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus_code\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mAPIConnectionError\u001b[0m: litellm.APIConnectionError: AzureException APIConnectionError - list index out of range\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.12/site-packages/litellm/main.py\", line 1194, in completion\n    optional_params = get_optional_params(\n                      ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.12/site-packages/litellm/utils.py\", line 3766, in get_optional_params\n    optional_params = litellm.AzureOpenAIConfig().map_openai_params(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.12/site-packages/litellm/llms/azure/chat/gpt_transformation.py\", line 165, in map_openai_params\n    api_version_month = api_version_times[1]\n                        ~~~~~~~~~~~~~~~~~^^^\nIndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "results = agent(\"What time is it in Seattle? And how is the weather?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysing the agent's results\n",
    "\n",
    "Nice! We've invoked our agent for the first time! Let's now explore the results object. First thing we can see is the messages being exchanged by the agent in the agent's object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-30T20:19:51.954608Z",
     "iopub.status.idle": "2025-07-30T20:19:51.954844Z",
     "shell.execute_reply": "2025-07-30T20:19:51.954724Z",
     "shell.execute_reply.started": "2025-07-30T20:19:51.954714Z"
    }
   },
   "outputs": [],
   "source": [
    "agent.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can take a look at the usage of our agent for the last query by analysing the result `metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-30T20:19:51.955579Z",
     "iopub.status.idle": "2025-07-30T20:19:51.955787Z",
     "shell.execute_reply": "2025-07-30T20:19:51.955696Z",
     "shell.execute_reply.started": "2025-07-30T20:19:51.955686Z"
    }
   },
   "outputs": [],
   "source": [
    "results.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations!\n",
    "\n",
    "In this notebook you learned how to use LiteLLM with OpeanAi serving answers for weather agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
