{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZJ0MgwY-xWk"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://arize.com/docs/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-2w57bhem8-hq24MB6u7yE_ZF_ilOYSBw#/shared-invite/email\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">LLM Ops - Tracing, Evaluation, and Analysis</h1>\n",
    "\n",
    "In this tutorial we will learn how to build, observe, evaluate, and analyze a LLM powered application.\n",
    "\n",
    "It has the following sections:\n",
    "\n",
    "1. Understanding LLM-powered applications\n",
    "2. Observing the application using traces\n",
    "3. Evaluating the application using LLM Evals\n",
    "4. Exploring and and troubleshooting the application using UMAP projection and clustering\n",
    "\n",
    "‚ö†Ô∏è This tutorial requires an OpenAI key to run\n",
    "\n",
    "\n",
    "## Understanding LLM powered applications\n",
    "\n",
    "Building software with LLMs, or any machine learning model, is [fundamentally different](https://karpathy.medium.com/software-2-0-a64152b37c35). Rather than compiling source code into binary to run a series of commands, we need to navigate datasets, embeddings, prompts, and parameter weights to generate consistent accurate results. LLM outputs are probabilistic and therefore don't produce the same deterministic outcome every time.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Arize-ai/phoenix-assets/main/images/blog/5_steps_of_building_llm_app.png\" width=\"1100\" />\n",
    "\n",
    "There's a lot that can go into building an LLM application, but let's focus on the architecture. Below is a diagram of one possible architecture. In this diagram we see that the application is built around an LLM but that there are many other components that are needed to make the application work.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Arize-ai/phoenix-assets/main/images/blog/llm_app_architecture.png\" width=\"1100\" />\n",
    "\n",
    "The complexity that is involved in building an LLM application is why observability is so important. Observability is the ability to understand the internal state of a system by examining its inputs and outputs. Each step of the response generation process needs to be monitored, evaluated and tuned to provide the best possible experience. Not only that, certain tradeoffs might need to be made to optimize for speed, cost, or accuracy. In the context of LLM applications, we need to be able to understand the internal state of the system by examining telemetry data such as LLM Traces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RTD44Cz-xWm"
   },
   "source": [
    "## Observing the application using traces\n",
    "\n",
    "LLM Traces and Observability lets us understand the system from the outside, by letting us ask questions about that system without knowing its inner workings. Furthermore, it allows us to easily troubleshoot and handle novel problems (i.e. ‚Äúunknown unknowns‚Äù), and helps us answer the question, ‚ÄúWhy is this happening?‚Äù\n",
    "\n",
    "LLM Traces are designed to be a category of telemetry data that is used to understand the execution of LLMs and the surrounding application context such as retrieval from vector stores and the usage of external tools such as search engines or APIs. It lets you understand the inner workings of the individual steps your application takes wile also giving you visibility into how your system is running and performing as a whole.\n",
    "\n",
    "Traces are made up of a sequence of `spans`. A span represents a unit of work or operation (think a span of time). It tracks specific operations that a request makes, painting a picture of what happened during the time in which that operation was executed.\n",
    "\n",
    "LLM Tracing supports the following span kinds:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Arize-ai/phoenix-assets/main/images/blog/span_kinds.png\" width=\"1100\"/>\n",
    "\n",
    "\n",
    "By capturing the building blocks of your application while it's running, phoenix can provide a more complete picture of the inner workings of your application. To illustrate this, let's look at an example LLM application and inspect it's traces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsE86Ku9-xWm"
   },
   "source": [
    "### Traces and Spans in action\n",
    "\n",
    "Let's build a relatively simple LLM-powered application that will answer questions about Arize AI. This example uses LlamaIndex for RAG and OpenAI as the LLM but you could use any LLM you would like. The details of the application are not important, but the architecture is. Let's get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZKZ1xwN-xWm"
   },
   "outputs": [],
   "source": [
    "%pip install -Uqq arize-phoenix llama-index-llms-openai llama-index-embeddings-openai \"openai>=1\" gcsfs nest_asyncio openinference-instrumentation-llama_index"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PiP4TcCc-xWm",
    "ExecuteTime": {
     "end_time": "2025-08-02T14:35:48.829133Z",
     "start_time": "2025-08-02T14:35:48.809824Z"
    }
   },
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "#     openai_api_key = getpass(\"üîë Enter your OpenAI API key: \")\n",
    "\n",
    "print(os.environ[\"OPENAI_API_KEY\"])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-proj-SYuMaDaKCwClbPf1GuuLT3BlbkFJerE2wmuMAK2gh1J2hfJM\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jsIRP_MW-xWn",
    "ExecuteTime": {
     "end_time": "2025-08-02T14:36:09.669111Z",
     "start_time": "2025-08-02T14:36:06.033319Z"
    }
   },
   "source": [
    "import phoenix as px\n",
    "\n",
    "px.launch_app().view()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linghuang/miniconda3/envs/llmops/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "‚ö†Ô∏è PHOENIX_COLLECTOR_ENDPOINT is set to http://localhost:6006/v1/traces.\n",
      "‚ö†Ô∏è This means that traces will be sent to the collector endpoint and not this app.\n",
      "‚ö†Ô∏è If you would like to use this app to view traces, please unset this environmentvariable via e.g. `del os.environ['PHOENIX_COLLECTOR_ENDPOINT']` \n",
      "‚ö†Ô∏è You will need to restart your notebook to apply this change.\n",
      "/Users/linghuang/miniconda3/envs/llmops/lib/python3.10/contextlib.py:142: SAWarning: Skipped unsupported reflection of expression-based index ix_cumulative_llm_token_count_total\n",
      "  next(self.gen)\n",
      "/Users/linghuang/miniconda3/envs/llmops/lib/python3.10/contextlib.py:142: SAWarning: Skipped unsupported reflection of expression-based index ix_latency\n",
      "  next(self.gen)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754145369.658037  108620 chttp2_server.cc:1682] UNKNOWN:No address added out of total 1 resolved for '[::]:4317' {created_time:\"2025-08-02T10:36:09.657853-04:00\", children:[UNKNOWN:Failed to add any wildcard listeners {created_time:\"2025-08-02T10:36:09.657772-04:00\", children:[UNKNOWN:Unable to configure socket {fd:87, created_time:\"2025-08-02T10:36:09.656769-04:00\", children:[UNKNOWN:bind: Address already in use (48) {created_time:\"2025-08-02T10:36:09.656523-04:00\"}]}, UNKNOWN:Unable to configure socket {fd:87, created_time:\"2025-08-02T10:36:09.657771-04:00\", children:[UNKNOWN:bind: Address already in use (48) {created_time:\"2025-08-02T10:36:09.657769-04:00\"}]}]}]}\n",
      "ERROR:    Traceback (most recent call last):\n",
      "  File \"/Users/linghuang/miniconda3/envs/llmops/lib/python3.10/site-packages/starlette/routing.py\", line 694, in lifespan\n",
      "    async with self.lifespan_context(app) as maybe_state:\n",
      "  File \"/Users/linghuang/miniconda3/envs/llmops/lib/python3.10/contextlib.py\", line 199, in __aenter__\n",
      "    return await anext(self.gen)\n",
      "  File \"/Users/linghuang/miniconda3/envs/llmops/lib/python3.10/site-packages/fastapi/routing.py\", line 134, in merged_lifespan\n",
      "    async with original_context(app) as maybe_original_state:\n",
      "  File \"/Users/linghuang/miniconda3/envs/llmops/lib/python3.10/contextlib.py\", line 199, in __aenter__\n",
      "    return await anext(self.gen)\n",
      "  File \"/Users/linghuang/miniconda3/envs/llmops/lib/python3.10/site-packages/fastapi/routing.py\", line 134, in merged_lifespan\n",
      "    async with original_context(app) as maybe_original_state:\n",
      "  File \"/Users/linghuang/miniconda3/envs/llmops/lib/python3.10/contextlib.py\", line 199, in __aenter__\n",
      "    return await anext(self.gen)\n",
      "  File \"/Users/linghuang/miniconda3/envs/llmops/lib/python3.10/site-packages/fastapi/routing.py\", line 134, in merged_lifespan\n",
      "    async with original_context(app) as maybe_original_state:\n",
      "  File \"/Users/linghuang/miniconda3/envs/llmops/lib/python3.10/contextlib.py\", line 199, in __aenter__\n",
      "    return await anext(self.gen)\n",
      "  File \"/Users/linghuang/miniconda3/envs/llmops/lib/python3.10/site-packages/fastapi/routing.py\", line 134, in merged_lifespan\n",
      "    async with original_context(app) as maybe_original_state:\n",
      "  File \"/Users/linghuang/miniconda3/envs/llmops/lib/python3.10/contextlib.py\", line 199, in __aenter__\n",
      "    return await anext(self.gen)\n",
      "  File \"/Users/linghuang/miniconda3/envs/llmops/lib/python3.10/site-packages/phoenix/server/app.py\", line 559, in lifespan\n",
      "    await stack.enter_async_context(grpc_server)\n",
      "  File \"/Users/linghuang/miniconda3/envs/llmops/lib/python3.10/contextlib.py\", line 619, in enter_async_context\n",
      "    result = await _cm_type.__aenter__(cm)\n",
      "  File \"/Users/linghuang/miniconda3/envs/llmops/lib/python3.10/site-packages/phoenix/server/grpc_server.py\", line 108, in __aenter__\n",
      "    server.add_insecure_port(f\"[::]:{get_env_grpc_port()}\")\n",
      "  File \"/Users/linghuang/miniconda3/envs/llmops/lib/python3.10/site-packages/grpc/aio/_server.py\", line 102, in add_insecure_port\n",
      "    return _common.validate_port_binding_result(\n",
      "  File \"/Users/linghuang/miniconda3/envs/llmops/lib/python3.10/site-packages/grpc/_common.py\", line 181, in validate_port_binding_result\n",
      "    raise RuntimeError(_ERROR_MESSAGE_PORT_BINDING_FAILED % address)\n",
      "RuntimeError: Failed to bind to address [::]:4317; set GRPC_VERBOSITY=debug environment variable to see detailed error message.\n",
      "\n",
      "ERROR:    Application startup failed. Exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://arize.com/docs/phoenix\n",
      "üì∫ Opening a view to the Phoenix app. The app is running at http://localhost:6006/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x3001af430>"
      ],
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"1000\"\n",
       "            src=\"http://localhost:6006/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vzjbjVwL-xWn",
    "ExecuteTime": {
     "end_time": "2025-08-02T14:37:24.859210Z",
     "start_time": "2025-08-02T14:37:24.369677Z"
    }
   },
   "source": [
    "from openinference.instrumentation.langchain import LangChainInstrumentor\n",
    "\n",
    "from phoenix.otel import register\n",
    "\n",
    "tracer_provider = register(\n",
    "    project_name=\"llmops\",\n",
    "    endpoint=\"http://localhost:6006/v1/traces\",\n",
    "    auto_instrument=True,\n",
    "    batch=True,\n",
    "    verbose=True,\n",
    ")\n",
    "tracer = tracer_provider.get_tracer(__name__)\n",
    "LangChainInstrumentor(tracer_provider=tracer_provider).instrument(skip_dep_check=True)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to instrument while already instrumented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî≠ OpenTelemetry Tracing Details üî≠\n",
      "|  Phoenix Project: llmops\n",
      "|  Span Processor: BatchSpanProcessor\n",
      "|  Collector Endpoint: http://localhost:6006/v1/traces\n",
      "|  Transport: HTTP + protobuf\n",
      "|  Transport Headers: {}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-Hf53mHt-xWn",
    "ExecuteTime": {
     "end_time": "2025-08-02T14:38:57.668749Z",
     "start_time": "2025-08-02T14:38:56.851594Z"
    }
   },
   "source": [
    "# --- LangChain core & community ---\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "sample_docs = [\n",
    "    Document(page_content=\"You can query monitor status using the GraphQL API at /v1/graphql/monitor\"),\n",
    "    Document(page_content=\"Delete a model using the `deleteModel` mutation in the GraphQL API\"),\n",
    "    Document(page_content=\"Enterprise license pricing is customized. Contact Arize support.\"),\n",
    "    Document(page_content=\"Log a prediction using the Python SDK with `log_prediction(model_id=..., features=...)`\"),\n",
    "]\n",
    "\n",
    "# Chunk the docs (and actually use the chunks)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(sample_docs)\n",
    "\n",
    "# --- Embeddings + Vector Store ---\n",
    "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")  # needs OPENAI_API_KEY\n",
    "vs = FAISS.from_documents(docs, embedding=emb)  # use from_documents for Document[]\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# --- LLM + QA chain ---\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    return_source_documents=True,  # optional: helpful for debugging\n",
    ")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRAA3Yfp-xWn"
   },
   "source": [
    "Now that we have an application setup, let's take a look inside."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kUvvi3Oa-xWn",
    "ExecuteTime": {
     "end_time": "2025-08-02T14:39:37.214037Z",
     "start_time": "2025-08-02T14:39:29.267607Z"
    }
   },
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "queries = [\n",
    "    \"How can I query for a monitor's status using GraphQL?\",\n",
    "    \"How do I delete a model?\",\n",
    "    \"How much does an enterprise license of Arize cost?\",\n",
    "    \"How do I log a prediction using the python SDK?\",\n",
    "]\n",
    "\n",
    "for q in tqdm(queries):\n",
    "    out = qa.invoke({\"query\": q})  # more reliable than passing a bare string\n",
    "    # Some chains return a dict with \"result\", others return a bare string.\n",
    "    result = out[\"result\"] if isinstance(out, dict) and \"result\" in out else out\n",
    "    print(f\"Query: {q}\\nResponse: {result}\\n\")\n",
    "\n",
    "    # Print sources only if present\n",
    "    sources = out.get(\"source_documents\", []) if isinstance(out, dict) else []\n",
    "    for i, d in enumerate(sources, 1):\n",
    "        print(f\"  Source {i}: {d.page_content[:120]}...\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|‚ñà‚ñà‚ñå       | 1/4 [00:01<00:05,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How can I query for a monitor's status using GraphQL?\n",
      "Response: You can query for a monitor's status using the GraphQL API at the endpoint `/v1/graphql/monitor`.\n",
      "\n",
      "  Source 1: You can query monitor status using the GraphQL API at /v1/graphql/monitor...\n",
      "  Source 2: Delete a model using the `deleteModel` mutation in the GraphQL API...\n",
      "  Source 3: Log a prediction using the Python SDK with `log_prediction(model_id=..., features=...)`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:03<00:03,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How do I delete a model?\n",
      "Response: You can delete a model using the `deleteModel` mutation in the GraphQL API.\n",
      "\n",
      "  Source 1: Delete a model using the `deleteModel` mutation in the GraphQL API...\n",
      "  Source 2: Log a prediction using the Python SDK with `log_prediction(model_id=..., features=...)`...\n",
      "  Source 3: You can query monitor status using the GraphQL API at /v1/graphql/monitor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:04<00:01,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How much does an enterprise license of Arize cost?\n",
      "Response: Enterprise license pricing is customized. You would need to contact Arize support for specific pricing information.\n",
      "\n",
      "  Source 1: Enterprise license pricing is customized. Contact Arize support....\n",
      "  Source 2: You can query monitor status using the GraphQL API at /v1/graphql/monitor...\n",
      "  Source 3: Delete a model using the `deleteModel` mutation in the GraphQL API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:07<00:00,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How do I log a prediction using the python SDK?\n",
      "Response: You can log a prediction using the Python SDK with the following code:\n",
      "\n",
      "```python\n",
      "log_prediction(model_id=..., features=...)\n",
      "```\n",
      "\n",
      "Make sure to replace `...` with the appropriate model ID and features for your prediction.\n",
      "\n",
      "  Source 1: Log a prediction using the Python SDK with `log_prediction(model_id=..., features=...)`...\n",
      "  Source 2: You can query monitor status using the GraphQL API at /v1/graphql/monitor...\n",
      "  Source 3: Delete a model using the `deleteModel` mutation in the GraphQL API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84tXu06W-xWn"
   },
   "source": [
    "Now that we've run the application a few times, let's take a look at the traces in the UI"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pZo_6DxH-xWn",
    "ExecuteTime": {
     "end_time": "2025-08-02T14:39:41.160953Z",
     "start_time": "2025-08-02T14:39:41.059721Z"
    }
   },
   "source": [
    "print(\"The Phoenix UI:\", px.active_session().url)"
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'url'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe Phoenix UI:\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[43mpx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mactive_session\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murl\u001B[49m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'url'"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-2p8Vy9-xWn"
   },
   "source": [
    "The UI will give you an interactive troubleshooting experience. You can sort, filter, and search for traces. You can also view the detail of each trace to better understand what happened during the response generation process.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/images/trace_details_view.png\" width=\"1100\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYgc489f-xWn"
   },
   "source": [
    "In addition to being able to view the traces in the UI, you can also query for traces to use as pandas dataframes in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ubbI-x3o-xWn",
    "ExecuteTime": {
     "end_time": "2025-08-02T14:40:03.857330Z",
     "start_time": "2025-08-02T14:40:03.620935Z"
    }
   },
   "source": [
    "spans_df = px.Client().get_spans_dataframe()\n",
    "spans_df[[\"name\", \"span_kind\", \"attributes.input.value\", \"attributes.retrieval.documents\"]].head()"
   ],
   "outputs": [
    {
     "ename": "HTTPStatusError",
     "evalue": "Client error '405 Method Not Allowed' for url 'http://localhost:6006/v1/traces/v1/spans?project_name=llmops&project-name=llmops'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHTTPStatusError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m spans_df \u001B[38;5;241m=\u001B[39m \u001B[43mpx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mClient\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_spans_dataframe\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m spans_df[[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspan_kind\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattributes.input.value\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattributes.retrieval.documents\u001B[39m\u001B[38;5;124m\"\u001B[39m]]\u001B[38;5;241m.\u001B[39mhead()\n",
      "File \u001B[0;32m~/miniconda3/envs/llmops/lib/python3.10/site-packages/phoenix/session/data_extractor.py:46\u001B[0m, in \u001B[0;36mTraceDataExtractor.get_spans_dataframe\u001B[0;34m(self, filter_condition, start_time, end_time, limit, root_spans_only, project_name, timeout)\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mget_spans_dataframe\u001B[39m(\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     35\u001B[0m     filter_condition: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     42\u001B[0m     timeout: Optional[\u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m DEFAULT_TIMEOUT_IN_SECONDS,\n\u001B[1;32m     43\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[pd\u001B[38;5;241m.\u001B[39mDataFrame]:\n\u001B[1;32m     44\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[1;32m     45\u001B[0m         Optional[pd\u001B[38;5;241m.\u001B[39mDataFrame],\n\u001B[0;32m---> 46\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery_spans\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     47\u001B[0m \u001B[43m            \u001B[49m\u001B[43mSpanQuery\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwhere\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilter_condition\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     48\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstart_time\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstart_time\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     49\u001B[0m \u001B[43m            \u001B[49m\u001B[43mend_time\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mend_time\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     50\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlimit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlimit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m            \u001B[49m\u001B[43mroot_spans_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mroot_spans_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m            \u001B[49m\u001B[43mproject_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproject_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m     55\u001B[0m     )\n",
      "File \u001B[0;32m~/miniconda3/envs/llmops/lib/python3.10/site-packages/phoenix/session/client.py:194\u001B[0m, in \u001B[0;36mClient.query_spans\u001B[0;34m(self, start_time, end_time, limit, root_spans_only, project_name, stop_time, timeout, orphan_span_as_root_span, *queries)\u001B[0m\n\u001B[1;32m    192\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m422\u001B[39m:\n\u001B[1;32m    193\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(response\u001B[38;5;241m.\u001B[39mcontent\u001B[38;5;241m.\u001B[39mdecode())\n\u001B[0;32m--> 194\u001B[0m \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    195\u001B[0m results \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    196\u001B[0m content_type \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mheaders\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mContent-Type\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/llmops/lib/python3.10/site-packages/httpx/_models.py:829\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    827\u001B[0m error_type \u001B[38;5;241m=\u001B[39m error_types\u001B[38;5;241m.\u001B[39mget(status_class, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid status code\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    828\u001B[0m message \u001B[38;5;241m=\u001B[39m message\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mself\u001B[39m, error_type\u001B[38;5;241m=\u001B[39merror_type)\n\u001B[0;32m--> 829\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m HTTPStatusError(message, request\u001B[38;5;241m=\u001B[39mrequest, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[0;31mHTTPStatusError\u001B[0m: Client error '405 Method Not Allowed' for url 'http://localhost:6006/v1/traces/v1/spans?project_name=llmops&project-name=llmops'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/405"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQ8xZf-Z-xWn"
   },
   "source": [
    "With just a few lines of code, we have managed to gain visibility into the inner workings of our application. We can now better understand how things like retrieval, prompts, and parameter weights could be affecting our application. But what can we do with this information? Let's take a look at how to use LLM evals to evaluate our application."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZbGGK6VK-xWo",
    "ExecuteTime": {
     "end_time": "2025-08-02T14:40:22.805183Z",
     "start_time": "2025-08-02T14:40:14.433487Z"
    }
   },
   "source": [
    "if (\n",
    "    input(\"The tutorial is about to move on to the evaluation section. Continue [Y/n]?\")\n",
    "    .lower()\n",
    "    .startswith(\"n\")\n",
    "):\n",
    "    assert False, \"notebook stopped\""
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjFLhL7u-xWo"
   },
   "source": [
    "## Evaluating the application using LLM Evals\n",
    "\n",
    "Evaluation should serve as the primary metric for assessing your application. It determines whether the app will produce accurate responses based on the data sources and range of queries.\n",
    "\n",
    "While it's beneficial to examine individual queries and responses, this approach is impractical as the volume of edge-cases and failures increases. Instead, it's more effective to establish a suite of metrics and automated evaluations. These tools can provide insights into overall system performance and can identify specific areas that may require scrutiny.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3u04b9Vc-xWo"
   },
   "source": [
    "Let's first convert our traces into a workable datasets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uRkTgB5M-xWo",
    "ExecuteTime": {
     "end_time": "2025-08-02T14:40:38.913213Z",
     "start_time": "2025-08-02T14:40:38.867823Z"
    }
   },
   "source": [
    "from phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\n",
    "\n",
    "retrieved_documents_df = get_retrieved_documents(px.active_session())\n",
    "queries_df = get_qa_with_reference(px.active_session())"
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'query_spans'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mphoenix\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msession\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mevaluation\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m get_qa_with_reference, get_retrieved_documents\n\u001B[0;32m----> 3\u001B[0m retrieved_documents_df \u001B[38;5;241m=\u001B[39m \u001B[43mget_retrieved_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mactive_session\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m queries_df \u001B[38;5;241m=\u001B[39m get_qa_with_reference(px\u001B[38;5;241m.\u001B[39mactive_session())\n",
      "File \u001B[0;32m~/miniconda3/envs/llmops/lib/python3.10/site-packages/phoenix/trace/dsl/helpers.py:65\u001B[0m, in \u001B[0;36mget_retrieved_documents\u001B[0;34m(obj, start_time, end_time, project_name, stop_time, timeout)\u001B[0m\n\u001B[1;32m     58\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m     59\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstop_time is deprecated. Use end_time instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     60\u001B[0m         \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m,\n\u001B[1;32m     61\u001B[0m     )\n\u001B[1;32m     62\u001B[0m     end_time \u001B[38;5;241m=\u001B[39m end_time \u001B[38;5;129;01mor\u001B[39;00m stop_time\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[1;32m     64\u001B[0m     pd\u001B[38;5;241m.\u001B[39mDataFrame,\n\u001B[0;32m---> 65\u001B[0m     \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery_spans\u001B[49m(\n\u001B[1;32m     66\u001B[0m         SpanQuery()\n\u001B[1;32m     67\u001B[0m         \u001B[38;5;241m.\u001B[39mwhere(IS_RETRIEVER)\n\u001B[1;32m     68\u001B[0m         \u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrace_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mINPUT)\n\u001B[1;32m     69\u001B[0m         \u001B[38;5;241m.\u001B[39mexplode(\n\u001B[1;32m     70\u001B[0m             RETRIEVAL_DOCUMENTS,\n\u001B[1;32m     71\u001B[0m             reference\u001B[38;5;241m=\u001B[39mDOCUMENT_CONTENT,\n\u001B[1;32m     72\u001B[0m             document_score\u001B[38;5;241m=\u001B[39mDOCUMENT_SCORE,\n\u001B[1;32m     73\u001B[0m         ),\n\u001B[1;32m     74\u001B[0m         start_time\u001B[38;5;241m=\u001B[39mstart_time,\n\u001B[1;32m     75\u001B[0m         end_time\u001B[38;5;241m=\u001B[39mend_time,\n\u001B[1;32m     76\u001B[0m         project_name\u001B[38;5;241m=\u001B[39mproject_name,\n\u001B[1;32m     77\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[1;32m     78\u001B[0m     ),\n\u001B[1;32m     79\u001B[0m )\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'query_spans'"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pidNmVOv-xWo"
   },
   "source": [
    "We can now use Phoenix's LLM Evals to evaluate these queries. LLM Evals uses an LLM to grade your application based on different criteria. For this example we will use the evals library to see if any `hallucinations` are present and if the `Q&A Correctness` is good (whether or not the application answers the question correctly)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RzUUui-9-xWo",
    "ExecuteTime": {
     "end_time": "2025-08-02T14:40:50.091226Z",
     "start_time": "2025-08-02T14:40:50.067662Z"
    }
   },
   "source": [
    "import nest_asyncio\n",
    "\n",
    "from phoenix.evals import (\n",
    "    HALLUCINATION_PROMPT_RAILS_MAP,\n",
    "    HALLUCINATION_PROMPT_TEMPLATE,\n",
    "    QA_PROMPT_RAILS_MAP,\n",
    "    QA_PROMPT_TEMPLATE,\n",
    "    OpenAIModel,\n",
    "    llm_classify,\n",
    ")\n",
    "\n",
    "nest_asyncio.apply()  # Speeds up OpenAI API calls\n",
    "\n",
    "# Check if the application has any indications of hallucinations\n",
    "hallucination_eval = llm_classify(\n",
    "    data=queries_df,\n",
    "    model=OpenAIModel(model=\"gpt-4o\", temperature=0.0),\n",
    "    template=HALLUCINATION_PROMPT_TEMPLATE,\n",
    "    rails=list(HALLUCINATION_PROMPT_RAILS_MAP.values()),\n",
    "    provide_explanation=True,  # Makes the LLM explain its reasoning\n",
    ")\n",
    "hallucination_eval[\"score\"] = (\n",
    "    hallucination_eval.label[~hallucination_eval.label.isna()] == \"factual\"\n",
    ").astype(int)\n",
    "\n",
    "# Check if the application is answering questions correctly\n",
    "qa_correctness_eval = llm_classify(\n",
    "    data=queries_df,\n",
    "    model=OpenAIModel(model=\"gpt-4o\", temperature=0.0),\n",
    "    template=QA_PROMPT_TEMPLATE,\n",
    "    rails=list(QA_PROMPT_RAILS_MAP.values()),\n",
    "    provide_explanation=True,  # Makes the LLM explain its reasoning\n",
    "    concurrency=4,\n",
    ")\n",
    "\n",
    "qa_correctness_eval[\"score\"] = (\n",
    "    qa_correctness_eval.label[~qa_correctness_eval.label.isna()] == \"correct\"\n",
    ").astype(int)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'queries_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 16\u001B[0m\n\u001B[1;32m     12\u001B[0m nest_asyncio\u001B[38;5;241m.\u001B[39mapply()  \u001B[38;5;66;03m# Speeds up OpenAI API calls\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Check if the application has any indications of hallucinations\u001B[39;00m\n\u001B[1;32m     15\u001B[0m hallucination_eval \u001B[38;5;241m=\u001B[39m llm_classify(\n\u001B[0;32m---> 16\u001B[0m     data\u001B[38;5;241m=\u001B[39m\u001B[43mqueries_df\u001B[49m,\n\u001B[1;32m     17\u001B[0m     model\u001B[38;5;241m=\u001B[39mOpenAIModel(model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt-4o\u001B[39m\u001B[38;5;124m\"\u001B[39m, temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0\u001B[39m),\n\u001B[1;32m     18\u001B[0m     template\u001B[38;5;241m=\u001B[39mHALLUCINATION_PROMPT_TEMPLATE,\n\u001B[1;32m     19\u001B[0m     rails\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlist\u001B[39m(HALLUCINATION_PROMPT_RAILS_MAP\u001B[38;5;241m.\u001B[39mvalues()),\n\u001B[1;32m     20\u001B[0m     provide_explanation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,  \u001B[38;5;66;03m# Makes the LLM explain its reasoning\u001B[39;00m\n\u001B[1;32m     21\u001B[0m )\n\u001B[1;32m     22\u001B[0m hallucination_eval[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscore\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     23\u001B[0m     hallucination_eval\u001B[38;5;241m.\u001B[39mlabel[\u001B[38;5;241m~\u001B[39mhallucination_eval\u001B[38;5;241m.\u001B[39mlabel\u001B[38;5;241m.\u001B[39misna()] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfactual\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     24\u001B[0m )\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mint\u001B[39m)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# Check if the application is answering questions correctly\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'queries_df' is not defined"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBM9IcdY-xWo"
   },
   "outputs": [],
   "source": [
    "hallucination_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aP82aa90-xWo"
   },
   "outputs": [],
   "source": [
    "qa_correctness_eval.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcqpR22j-xWo"
   },
   "source": [
    "As you can see from the results, one of the queries was flagged as a hallucination. Let's log these results to the phoenix server to view the hallucinations in the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xj8dHlPw-xWo"
   },
   "outputs": [],
   "source": [
    "from phoenix.trace import SpanEvaluations\n",
    "\n",
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval),\n",
    "    SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_correctness_eval),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXiNxseC-xWo"
   },
   "source": [
    "Now that we have the hallucinations logged, let's take a look at them in the UI. You will notice that the traces that correspond to hallucinations are clearly marked in the UI and you can now query for them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yvZYC0Yq-xWo"
   },
   "outputs": [],
   "source": [
    "print(\"The Phoenix UI:\", px.active_session().url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tviRs8G9-xWo"
   },
   "source": [
    "We've now identified that there are certain queries that are resulting in hallucinations or incorrect answers. Let's see if we can use LLM Evals to identify if these issues are caused by the retrieval process for RAG. We are going to use an LLM to grade whether or not the chunks retrieved are relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQO8w7EU-xWo"
   },
   "outputs": [],
   "source": [
    "from phoenix.evals import (\n",
    "    RAG_RELEVANCY_PROMPT_RAILS_MAP,\n",
    "    RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    OpenAIModel,\n",
    "    llm_classify,\n",
    ")\n",
    "\n",
    "retrieved_documents_eval = llm_classify(\n",
    "    data=retrieved_documents_df,\n",
    "    model=OpenAIModel(model=\"gpt-4o\", temperature=0.0),\n",
    "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
    "    provide_explanation=True,\n",
    ")\n",
    "\n",
    "retrieved_documents_eval[\"score\"] = (\n",
    "    retrieved_documents_eval.label[~retrieved_documents_eval.label.isna()] == \"relevant\"\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crH2bxwt-xWo"
   },
   "outputs": [],
   "source": [
    "retrieved_documents_eval.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZEOu_XS-xWo"
   },
   "source": [
    "Looks like we are getting a lot of irrelevant chunks of text that might be polluting the prompt sent to the LLM. Let's log these evals to phoenix, at which point phoenix will automatically calculate retrieval metrics for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VFUUc1-q-xWo"
   },
   "outputs": [],
   "source": [
    "from phoenix.trace import DocumentEvaluations\n",
    "\n",
    "px.Client().log_evaluations(\n",
    "    DocumentEvaluations(eval_name=\"Relevance\", dataframe=retrieved_documents_eval)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkBb15TJ-xWo"
   },
   "source": [
    "If we once again visit the UI, we will now see that Phoenix has aggregated up retrieval metrics (`precision`, `ndcg`, and `hit`). We see that our hallucinations and incorrect answers directly correlate to bad retrieval!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MoiGbT9Y-xWo"
   },
   "outputs": [],
   "source": [
    "print(\"The Phoenix UI:\", px.active_session().url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXK5Z6Rw-xWo"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Arize-ai/phoenix-assets/main/images/screenshots/document_evals_on_traces.png\" />\n",
    "\n",
    "There are many more evaluations metrics that can be used to make determinations about the quality of your application. Phoenix supports evaluating not only traces but individual spans (such as retrievers) as well as performing retrieval analysis for your document chunks. Evaluation metrics can also be customized for your specific use-case. For more details, consult the phoenix docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7yFRKsm-xWo"
   },
   "outputs": [],
   "source": [
    "if (\n",
    "    input(\"The tutorial is about to move on to the analysis section. Continue [Y/n]?\")\n",
    "    .lower()\n",
    "    .startswith(\"n\")\n",
    "):\n",
    "    assert False, \"notebook stopped\"\n",
    "\n",
    "px.close_app()  # Close the Phoenix UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2C8DS1I-xWo"
   },
   "source": [
    "## Exploring and and troubleshooting the application using UMAP projection and clustering\n",
    "\n",
    "We have so far figured out how to trace our application and how to evaluate it. But what if we need to understand the application at a higher level? What if we need to understand if the application is performing badly for a certain topic or if there are any patterns in the data that we can use to improve the application? This is where UMAP projection and clustering comes in. UMAP and clustering let's you view the query embeddings of your application in a 3D space. This allows you to see patterns in the data and understand how your application is performing in various clusters (semantic groups).\n",
    "\n",
    "Since we only ran the application for 4 queries, we won't be able to see any patterns in the data. Let's pull in a larger dataset of queries and run the application again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w56gFTY6-xWo"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Pull in queries from the LLM\n",
    "query_df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/llm/llama-index/arize-docs/query_data_complete3.parquet\",\n",
    ")\n",
    "\n",
    "query_ds = px.Inferences.from_open_inference(query_df)\n",
    "\n",
    "query_ds.dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmXBEPqr-xWu"
   },
   "source": [
    "Let's also pull in the embeddings for our knowledge base. This will allow us to see how how document chunks are being retrieved by the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RK1jrQ3f-xWv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def storage_context_to_dataframe(storage_context: StorageContext) -> pd.DataFrame:\n",
    "    \"\"\"Converts the storage context to a pandas dataframe.\n",
    "\n",
    "    Args:\n",
    "        storage_context (StorageContext): Storage context containing the index\n",
    "        data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataframe containing the index data.\n",
    "    \"\"\"\n",
    "    document_ids = []\n",
    "    document_texts = []\n",
    "    document_embeddings = []\n",
    "    docstore = storage_context.docstore\n",
    "    vector_store = storage_context.vector_store\n",
    "    for node_id, node in docstore.docs.items():\n",
    "        document_ids.append(node.hash)  # use node hash as the document ID\n",
    "        document_texts.append(node.text)\n",
    "        document_embeddings.append(np.array(vector_store.get(node_id)))\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"document_id\": document_ids,\n",
    "            \"text\": document_texts,\n",
    "            \"text_vector\": document_embeddings,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "database_df = storage_context_to_dataframe(storage_context)\n",
    "database_df = database_df.drop_duplicates(subset=[\"text\"])\n",
    "database_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pix4HTNa-xWv"
   },
   "source": [
    "Let's now launch Phoenix with these two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZaIOFl0-xWv"
   },
   "outputs": [],
   "source": [
    "# get a random sample of 500 documents (including retrieved documents)\n",
    "# this will be handled by by the application in a coming release\n",
    "num_sampled_point = 500\n",
    "retrieved_document_ids = set(\n",
    "    [\n",
    "        doc_id\n",
    "        for doc_ids in query_df[\":feature.[str].retrieved_document_ids:prompt\"].to_list()\n",
    "        for doc_id in doc_ids\n",
    "    ]\n",
    ")\n",
    "retrieved_document_mask = database_df[\"document_id\"].isin(retrieved_document_ids)\n",
    "num_retrieved_documents = len(retrieved_document_ids)\n",
    "num_additional_samples = num_sampled_point - num_retrieved_documents\n",
    "unretrieved_document_mask = ~retrieved_document_mask\n",
    "sampled_unretrieved_document_ids = set(\n",
    "    database_df[unretrieved_document_mask][\"document_id\"]\n",
    "    .sample(n=num_additional_samples, random_state=0)\n",
    "    .to_list()\n",
    ")\n",
    "sampled_unretrieved_document_mask = database_df[\"document_id\"].isin(\n",
    "    sampled_unretrieved_document_ids\n",
    ")\n",
    "sampled_document_mask = retrieved_document_mask | sampled_unretrieved_document_mask\n",
    "sampled_database_df = database_df[sampled_document_mask]\n",
    "\n",
    "database_schema = px.Schema(\n",
    "    prediction_id_column_name=\"document_id\",\n",
    "    prompt_column_names=px.EmbeddingColumnNames(\n",
    "        vector_column_name=\"text_vector\",\n",
    "        raw_data_column_name=\"text\",\n",
    "    ),\n",
    ")\n",
    "database_ds = px.Inferences(\n",
    "    dataframe=sampled_database_df,\n",
    "    schema=database_schema,\n",
    "    name=\"database\",\n",
    ")\n",
    "\n",
    "(session := px.launch_app(primary=query_ds, corpus=database_ds, run_in_thread=False)).View()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upGqa9Aa-xWv"
   },
   "source": [
    "Using Phoenix's embedding projection feature, we can now view the query embeddings in a 3D space. What you are looking at is a point-cloud where each point represents a query. When embeddings are projected into a lower dimensional space, UMAP preserves the semantic distance that the embeddings encode. This means that the points that are closer together are more similar. This allows us to see patterns in the data and understand how our application is performing in various clusters. If you click through the clusters, you will see clusters of queries that are significantly farther away from the knowledge base. This means that the knowledge base does not contain enough information to answer the user's question (In this case, the document chunks don't contain any information about pricing. Can you find this cluster?).\n",
    "\n",
    "Phoenix's embedding view also supports coloring and cluster metrics by evaluations! This means you can use it to find pockets of poor user feedback or hallucinations. Phoenix's embedding view supports embeddings for text, images, and video so a most forms of unstructured data can be analyzed.\n",
    "\n",
    "Last but not least, Phoenix can draw the connections between queries and the document chunks. This allows you to see which document chunks are being retrieved for each query and to visualize the semantic distance between them.\n",
    "\n",
    "<img src=\"https://github.com/Arize-ai/phoenix-assets/raw/main/gifs/corpus_search_and_retrieval.gif?raw=true\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSDn18a8-xWv"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Phoenix is a powerful companion for building LLM-powered applications. It allows you to observe, evaluate, and explore your application at every step of you development process. For more details, consult the [phoenix docs](https://arize.com/docs/phoenix)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
